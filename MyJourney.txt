Long Process Log (Development Journey)
Concept and Ambition: I began this journey with an ambitious idea in mind – turning my personal website into a living, interactive experience. It was late 2024 and I was reviewing my standard MERN portfolio site. I thought, why not let visitors actually converse with my portfolio? Having recently worked on an LLM chatbot during my co-op, I was inspired to apply that knowledge to my own project. The vision was clear: an AI-powered “me” that could answer questions 24/7. I knew this would be a complex undertaking (far beyond the typical student portfolio), but I was excited to push my limits. Planning the Architecture: To start, I sketched out how this AI companion would fit into my existing MERN stack. The site already had a Node/Express backend and a React frontend. I anticipated heavy I/O and wanted great performance for the chatbot, so I decided to swap in Fastify (a fast Node.js framework) for the backend API – a choice that promised better throughput than Express. I treated this like a full project on its own, planning out components and milestones to keep myself on track alongside schoolwork. I also prepared my data sources: my MongoDB collections (projects, experiences, etc.), my resume PDF, and GitHub API data. From the get-go, I set up an in-memory cache plan for quick data access and identified that I’d need new endpoints for AI interactions. Laying this groundwork upfront helped me break down the daunting project into manageable pieces. Building the Knowledge Base: The first major task was compiling the knowledge my AI would use. I wrote an aggregate script to pull content from everywhere. This meant querying my portfolio’s MongoDB for all project descriptions, experience entries, involvements, etc., reading and parsing my resume PDF (using a library called pdf-parse), and calling the GitHub API to list my repos and fetch snippets of READMEs. I remember debugging a lot in this phase – e.g., handling edge cases like missing descriptions or cleaning out HTML tags from text. Eventually, I had a pretty large JSON of “everything about Kartavya.” I then split this content into smaller, meaningful chunks. This was tricky: I had to decide how much info each chunk should hold. In the end I went with chunks roughly the size of a paragraph or bullet list each, often corresponding to one project or one section of my resume. This granularity felt right for the next step: vector embeddings. Indexing with Vector Embeddings: To enable semantic search, I needed to convert each text chunk into a vector representation. I used OpenAI’s embedding API (their text-embedding-3-small model) to get a 1536-dimension vector for each piece of text. Seeing this work was gratifying – suddenly I had a numerical fingerprint for every line of my experience! But it also taught me about scale and cost: I had dozens and dozens of chunks, and embedding them all takes time and money. To be efficient, I implemented a strategy to cache these embeddings and not redo them unless necessary. I even set a rule: only rebuild the vector index at most once per month, or when I update content. This caching paid off; during development I could restart the server without re-embedding everything each time. I stored all these vectors in a MongoDB collection (leveraging MongoDB Atlas’s vector search capabilities). This was a crucial pivot – initially I considered an external vector DB or a third-party semantic search API, but by using Atlas Search I kept everything in-house. I also wrote a quick script to verify the index by manually querying a few known terms (e.g., “React” or “Cincinnati”) to ensure the right chunks came up. That moment – when a query vector pulled back a relevant snippet from my resume – was pure magic and affirmed that this approach would work. Retrieval and Ranking Algorithm: With an embedded knowledge base ready, the next challenge was designing how the AI fetches relevant info during a conversation. I knew a simple nearest-neighbor search could retrieve relevant chunks for a query, but I wanted finer control. I ended up developing a custom ranking algorithm. First, for any question asked, the system generates an embedding for the query itself (again using OpenAI’s API). It then computes cosine similarities between that query vector and all the stored vectors. Rather than blindly trust those scores, I introduced category weights – for example, if the query mentions “project” or matches a project name, I boost the project data’s scores. I did similar boosting for resume content if certain keywords (like “education” or “skills”) appear. Conversely, I down-weighted categories if they seemed irrelevant (no need to pull in, say, honors from the database if the question is about work experience). This was a lot of trial and error: I’d pose a question, see which chunks were getting selected, and adjust the weighting logic or thresholds. I also experimented with Maximal Marginal Relevance (MMR) to ensure diversity in the context – I wrote a function to iteratively pick the next best chunk that adds new info rather than duplicating what’s already selected. Ultimately, the retrieval pipeline became pretty robust: for a given question, the system might pull, say, 2 project blurbs, 1 experience summary, and 1 snippet from my resume as context. Achieving that balance where the AI gets a well-rounded brief for each query was challenging but incredibly satisfying. Conversational AI Backend: Next, I built out the backend endpoints and logic to handle actual question-answering. I created a new Fastify route /api/ai/ask-chat that the frontend would call with a user’s question. When a request hits this route, the server performs the vector search and ranking as described, then packages the top relevant text chunks into a prompt for the AI. Crafting the prompt was another thoughtful process: I decided on a format where I provide the AI a system message like “You are Kartavya Singh… (with guidelines on tone and knowledge)” and then a user message that includes a block of “Context” with numbered sources and the actual question. This technique is known as Retrieval-Augmented Generation (RAG), and it helps ground the AI’s answer in real data. I chose the OpenAI GPT-4 model for generating answers because of its reliability and coherence (using a smaller model earlier led to some iffy responses). The first time I got a full answer back from the AI, referencing my project names correctly and speaking as if it were me, I literally smiled ear to ear. Of course, there were bugs to fix at this stage – e.g., I had to strip any newlines or odd characters from the context to avoid confusing the prompt, and I had to enforce a max tokens limit so the AI wouldn’t ramble on. I also implemented two additional endpoints: one for follow-up question suggestions and one for conversation memory updates. These were lightweight calls (also to OpenAI) that I’ll describe when talking about the frontend. With these pieces, my backend was capable of handling a full conversational cycle. Frontend Integration and UI/UX: With the backend working, I shifted focus to the React frontend. I added a new AI Chatbot widget to the site’s interface – a little chat icon that, when clicked, opens a chat window overlay. Designing this UI was a project in itself. I wanted it to feel friendly and unobtrusive. I placed my profile picture in the chat header to personify the bot, and used a clean chat bubble design for messages. Using React hooks, I managed the state for the chat history, the current input, loading statuses, etc. One feature I’m proud of is the speech-to-text input: I integrated the Web Speech API so that if you click a microphone button, you can speak your question. The spoken words appear in real-time (this took some finesse with interim transcript events) and then send off just like a typed question. This makes the experience feel futuristic and accessible. Another nice touch is the typing indicator – when the AI is “thinking,” I update its message bubble to say things like “Gathering context...” and “Generating response…” with a little animated ellipsis. It’s a small detail, but it reassures the user that the system is working on a reply (rather than stuck). Once the answer comes back, I even implemented a typewriter effect to display it character by character, which makes the reveal more engaging than a sudden block of text. Maintaining Conversation Context: An important aspect of the chat is that it’s conversational – you can ask a follow-up question and the AI should remember what you’re talking about. Achieving this was another hurdle. I opted not to send the entire chat history to the server (to avoid huge prompts and costs). Instead, I developed a conversation memory strategy. After each question-answer exchange, I call an endpoint that summarizes the exchange into 2-3 sentences of “memory.” The next time you ask something, the client includes this summary (not the full history) along with the new question. The summary is updated cumulatively. For example, if the user asked “Tell me about your internship at XYZ” and got an answer, the memory might store “User and Kartavya discussed his XYZ internship, including his role and key learnings.” If the next question is “What about personal projects?”, the system has that memory to know the context of “he” refers to me and maybe avoid repeating the internship info. Implementing this was pretty novel for me: I basically wrote a mini-prompt that instructs GPT-4 to act as a memory compressor. It took a few tries to get useful summaries (sometimes it would default to very generic statements). But once tuned, this approach worked wonderfully – it keeps the context without overloading the conversation. Follow-Up Suggestions Feature: During development, I thought it would be cool to offer the user some suggested questions, kind of like how some chatbots or search engines do. So I built a follow-up suggestion feature. After the AI answers a question, the frontend asynchronously calls /api/ai/suggestFollowUpQuestions with the last Q&A pair. The backend uses GPT-4 again with a prompt like “Given this Q&A, suggest 3 follow-up questions the user might ask next.” The suggestions (e.g., “What did you learn from that project?”) come back as a simple list of strings. I display these as clickable chips below the AI’s answer. This was a fun add-on that makes the chat feel more interactive – it can gently guide a conversation or help showcase areas of my portfolio the user might not think to ask about. Technically, juggling this additional call in the frontend required careful promise handling (I didn’t want it to slow down the main answer). I ended up managing it by initiating the suggestion API call the moment I receive the main answer, and then awaiting it a bit later, after the answer has been rendered. It’s non-blocking and works smoothly. Testing and Iteration: Once the full loop (question -> answer -> follow-ups -> memory update) was in place, I went through extensive testing. I peppered the AI with all sorts of questions: factual ones like “How many projects does Kartavya have?” (to test retrieval accuracy), open-ended ones like “What’s Kartavya passionate about?” (to test how well the persona and context come through), and curveballs like “Who’s the President of the US?” (to ensure it knows to say it’s out of scope). This testing phase revealed a few issues. For instance, early on, the AI tended to sometimes include information that wasn’t directly asked, just because it was in the context. To fix that, I refined the system prompt to encourage concise answers and to only use the provided context. I also adjusted how many context chunks I allowed – too many and the AI might get distracted by irrelevant info, too few and it might miss something important. Finding that sweet spot was an iterative process; I settled on a maximum of ~15 sentences worth of context in prompt. Another interesting bug: the conversation memory sometimes made the AI too brief, because each time it summarized, a little detail was lost. I addressed this by tweaking the prompt for memory generation to ensure it retained proper nouns and specific facts. Over multiple rounds of testing (and even asking friends to try it out), the system became more robust and accurate. Performance and Stress Management: Throughout development, I paid attention to performance – both of the app and my own. On the app side, I implemented logging for each request to monitor latency and resource usage. I simulated multiple users by opening several browser instances and firing questions simultaneously. I was happy to see the in-memory cache and Fastify held up well: even when a lot was happening, response times stayed within a few seconds. Memory usage on the server was stable, thanks to not storing long chat histories server-side. On my side as a developer, this project was a marathon that really tested my time and stress management. I had moments where I felt stuck (like when the embedding building took forever, or when the UI state got messy) and it was tempting to either rush a hacky solution or step away entirely. I managed these moments by staying organized – I kept a journal of bugs and ideas, prioritized them, and tackled them one by one. I also made sure to not burn out; there were evenings I forced myself to close the code editor and get some rest, which in hindsight saved me from diminishing returns of overtired coding. It’s a great feeling to come back the next day fresh and solve in 30 minutes what was baffling me the night before. Reflection on Outcome: Completing “My AI Companion” has been incredibly rewarding. I turned a lofty idea into a working reality, and in the process, I learned so much about building and integrating AI systems end-to-end. It reinforced the value of having high standards – I didn’t want to settle for “okay, it kind of works.” I pushed until the experience was smooth and polished. That said, it also taught me the art of balancing perfection with practicality; sometimes you have to declare it “good enough for now” and iterate later. The positive feedback I’ve received – peers telling me “this is so cool and unique!” – makes all the effort worth it. It’s not common for a student portfolio to have an AI chatbot, and I’m glad I dared to try this out. This project not only showcases my technical skills (from full-stack development to AI engineering), but also my ability to lead a complex project independently, manage my time, and persist through challenges. It’s a project I’ll fondly remember as I move forward, and it’s definitely raised the bar for whatever I work on next. I’m excited to keep improving the companion and to carry these lessons into my future endeavors.